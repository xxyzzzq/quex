.. _sec-token-policies:

Token Passing Policies
======================

The primary result result of a lexical analysis step is a so called 'token
identifier', that tells how the current input can be catagorized, e.g. as a
'KEYWORD', a 'NUMBER', or an 'IP_ADDRESS'. Additionally some information about
the actual stream content might have to be stored. For example when a 'STRING'
is identified the content of the string might be interesting for the caller of
the lexical analyzer. This basic information needs to be passed to the user of
the lexical analyzer. The following subsections describe the two token passing
policies available in quex: ``queue`` and ``single``.

Queue
-----

Command line option: ``--token-policy  queue``, or no ``--token-policy``. 

This is the policy that is used if no other one is specified. It has multiple
advantages:
  
   -- Using a token queue enables the sending of multiple
      tokens as response to a single pattern match. 
      
   -- The events ``on_entry``, ``on_exit``, and ``on_indentation`` 
      can be used without much consideration of what happens to the 
      tokens. 
      
   -- Finally, multiple pattern matches can be performed without 
      returning from the lexical analyzer function.

All of this comes with the slight disadvantage that the token queue iterators
need to be increased and reset for an analyzis step. Since this overhead is
most likely outweight by the three advantages above, ``queue`` is the default
token. The decision wether the analyzis shall continue or return can be
made inside the pattern actions using ``RETURN`` and ``CONTINUE``.

When 'queue' is used as token passing policy, then the interaction with 
the lexical analyzer is simply to receive a pointer to the current
token, i.e.

   .. code-block:: cpp

      token_p = my_lexer.receive();

where the member function ``.receive()`` returns a pointer to the current token
inside the pre-allocated token array.  The call to the receive function either
reads remaining tokens in the token queue or triggers the next step of lexical
analyzis. All the management of the token queue is hidden inside the analyzer's
engine. 

The size of the token queue is constant during run-time. Quex generates the size
based on the the command line option ``--token-queue-size``. An overflow of the
queue is prevented since the analyser returns as soon as the queue is full. The
default size of the token queue is 64 tokens.

Then the ``--token-queue-safety-border`` command line flag allows to define a
safety region. Now, the analyzer returns as soon as the remaining free places
in the queue are less then the specified number. This ensures that for each
analyzis step there is a minimum of places in the queue to which it can write
tokens. In other words, the safety-border corresponds to the maximum number of
tokens send as reaction to a pattern match, including indentation events and
mode transitions.  The default safety border is 16 tokens.

Additionally, there are three functions for low-level interaction
with the token queue:

   .. code-block:: cpp

      bool   token_queue_is_empty();
      void   token_queue_remainder_get(QUEX_TYPE_TOKEN**  begin, 
                                       QUEX_TYPE_TOKEN**  end);
      void   token_queue_memory_get(QUEX_TYPE_TOKEN** memory, size_t* n);
      void   token_queue_memory_set(QUEX_TYPE_TOKEN*  Memory, size_t  N);

The first function returns ``true`` if the queue is empty and ``false`` if
not. The second function returns the remaining tokens. After a call to 
this function ``*Begin`` will point to the begin of the remaining tokens
and ``*End`` will point right behind the last token in the remainder. Thus,
it is possible to iterate over the remaining tokens with

   .. code-block:: cpp

      for(QUEX_TYPE_TOKEN* p = *begin; p != *end; ++p) {
          ...
      }

The function ``token_queue_memory_set(...)`` imposes a memory chunk that is to
be used as a token queue. ``Memory`` points to the begin of the memory and
``N`` indicates the number of tokens that the memory chunk can contain. This
action is intrusive and analyzer automatically releases ownership over the
token queue's memory. It is advisable to call ``token_queue_memory_get(...)``
where ``*memory`` will point to the begin of the token queue's memory and
``*n`` will contain the number of tokens that can be contained by it.

It is generally a good idea to check whether the token queue is empty before
imposing a new queue. That is

   .. code-block:: cpp

      if( my_lexer.token_queue_is_empty() ) {
          my_lexer.token_queue_memory_set(MyMemory, N);
      }

otherwise, one might risk to drop tokens that are lost inside the old queue
when the new queue came in. 

.. warning::

   The state of the lexical analyzer corresponds always to the last token in
   the token queue! The ``.receive()`` functions only take one token from the
   queue which is not necessarily the last one. If state elements, such as line
   numbers or indentation, need to be associated with tokens, token stamping
   needs to be used :ref:`sec-token-stamping`.

Single
------

Command line option: ``--token-policy  single``

When a single token is used many advantages are lost queue are lost. The
capability to send repeated tokens via a single token using ``self_send_n``
remains intact. However, only one distinct token can be sent as reaction to one
pattern match. Also, it must be taken care that the pattern actions that appear
around the events ``on_entry``, ``on_exit``, and ``on_indentation`` do not sent
tokens if the event handler's do. Otherwise, token content is simply overwritten
and lost. Also, if the content of a token is set by a pattern action, it is
generally a bad idea to use ``CONTINUE`` after sending a token. The subsequent
pattern action may corrupt the token content. 

   .. code-block:: cpp

      token_id = my_lexer.receive();

where the member function ``.receive()`` returns the identifier of the current
token. The current token can be accessed via

   .. code-block:: cpp
   
       token_p = my_lexer.token_p();

where the returned pointer always points to the same token object as long
as one does not call 

   .. code-block:: cpp
   
       my_lexer.token_p_set(&my_token);

where the passed argument must point to a prepared token object.

An advantage of the single token policy is that there is no increment 
operation for the iteration over the token array. Also, no token queue
reset needs to happen at the entry of the analyzis step. A decision whether
to take the risk and effort that comes with the single token policy
needs to be well considered. It is always a good idea to run some benchmarks
on a basic lexical analyzer implementation in order to see which policy
is preferrable. The ability of sending repeated tokens remains.

Remarks
-------

The passing of tokens from the analyzer is a straight-forward one-way
process. The life line of an object generated by the analyzer shall
look like

   #. Caller calls the lexical analyzer via ``.receive()``.
   #. Analyzer generates object.
   #. Object is stored inside the token.
   #. Caller takes the object.
   #. Caller deletes the object.

If this policy is followed consequently, then the token object itself does not
need any ownership management. As a consequence the token passing becomes both
faster and less memory consuming. The default token class used in quex,
however, implements a safe strategy. For example the ``text`` member
relies on C++ Standard Library's ``basic_string`` for strings.
