.. _sec-token-policies:

Token Passing Policies
======================

The primary result result of a lexical analysis step is a so called 'token
identifier', that tells how the current input can be catagorized, e.g. as a
'KEYWORD', a 'NUMBER', or an 'IP_ADDRESS'. Additionally some information about
the actual stream content might have to be stored. For example when a 'STRING'
is identified the content of the string might be interesting for the caller of
the lexical analyzer. This basic information needs to be passed to the user of
the lexical analyzer. The following subsections describe the two token passing
policies available in quex: ``queue`` and ``single``.

Queue
-----

Command line option: ``--token-policy  queue``, or no ``--token-policy``. 

Setting the command line parameter ``--token-policy`` followed by the string
``queue`` enables the 'queue' token passing policy. This is the default token
passing policy. It has multiple advantages:
  
   -- Using a token queue enables the sending of multiple
      tokens as response to a single pattern match. 
      
   -- The events ``on_entry``, ``on_exit``, and ``on_indentation`` 
      can be used without much consideration of what happens to the 
      tokens. 
      
   -- Finally, multiple pattern matches can be performed without 
      returning from the lexical analyzer function.

All of this comes with the slight disadvantage that the token queue iterators
need to be increased and reset for an analyzis step. Since this overhead is
most likely outweight by the three advantages above, ``queue`` is the default
token. The decision wether the analyzis shall continue or return can be
made inside the pattern actions using ``RETURN`` and ``CONTINUE``.

When 'queue' is used as token passing policy, then the interaction with 
the lexical analyzer is simply to receive a pointer to the current
token, i.e.

   .. code-block:: cpp

      token_p = my_lexer.receive();

where the member function ``.receive()`` returns a pointer to the current token
inside the pre-allocated token array.  The call to the receive function either
reads remaining tokens in the token queue or triggers the next step of lexical
analyzis. All the management of the token queue is hidden inside the analyzer's
engine. If one desires to provide quex with a 'private' token queue one can
pass it via

   .. code-block:: cpp

      my_lexer.token_queue_set(Begin, End);

where both, ``Begin`` and ``End`` are pointer of type ``QUEY_TYPE_TOKEN``.
``Begin`` points to the first token in the array. ``End`` points to the 
first address behind the last token in the array, i.e. ``End`` is equal
to ``Begin`` plus the size of the array. It is generally a good idea
to check whether the token queue is empty before imposing a new queue. That is

   .. code-block:: cpp

      if( my_lexer.token_queue.is_empty() ) {
          my_lexer.token_queue_set(Begin, End);
      }

otherwise, one might risk to drop tokens that are lost inside the old queue
when the new queue came in.

The size of the token queue is constant during run-time. Quex generates the size
based on the the command line option ``--token-queue-size``. An overflow of the
queue is prevented since the analyser returns as soon as the queue is full. The
default size of the token queue is 64 tokens.

Then the ``--token-queue-safety-border`` command line flag allows to define a
safety region. Now, the analyzer returns as soon as the remaining free places
in the queue are less then the specified number. This ensures that for each
analyzis step there is a minimum of places in the queue to which it can write
tokens. In other words, the safety-border corresponds to the maximum number of
tokens send as reaction to a pattern match, including indentation events and
mode transitions.  The default safety border is 16 tokens.

.. warning::

   When using token queue: The state of the lexical analyzer
   corresponds to the last token in the token queue! The ``.receive()``
   functions only take one token from the queue which is not necessarily the
   last one. If state elements, such as line numbers or indentation, need to be
   associated with tokens, token stamping needs to be used
   :ref:`sec-token-stamping`.

Single
------

Command line option: ``--token-policy  single``

When a single token is used many advantages are lost queue are lost. The
capability to send repeated tokens via a single token using ``self_send_n``
remains intact. However, only one distinct token can be sent as reaction to one
pattern match. Also, it must be taken care that the pattern actions that appear
around the events ``on_entry``, ``on_exit``, and ``on_indentation`` do not sent
tokens if the event handler's do. Otherwise, token content is simply overwritten
and lost. Also, if the content of a token is set by a pattern action, it is
generally a bad idea to use ``CONTINUE`` after sending a token. The subsequent
pattern action may corrupt the token content. 

   .. code-block:: cpp

      token_id = my_lexer.receive();

where the member function ``.receive()`` returns the identifier of the current
token. The current token can be accessed via

   .. code-block:: cpp
   
       token_p = my_lexer.token_p();

where the returned pointer always points to the same token object as long
as one does not call 

   .. code-block:: cpp
   
       my_lexer.token_p_set(&my_token);

where the passed argument must point to an prepared token object.

An advantage of the single token policy is that there is no increment 
operation for the iteration over the token array. Also, no token queue
reset needs to happen at the entry of the analyzis step. A decision whether
to take the risk and effort that comes with the single token policy
needs to be well considered. It is always a good idea to run some benchmarks
on a basic lexical analyzer implementation in order to see which policy
is preferrable. The ability of sending repeated tokens remains.

Remarks
-------

The information contained in a token *is not* owned by the token itself, in a
sense that it is *not* responsible for de-allocating its content. A token only
carries values and pointers from the lexical analyzer to its caller and the
ownership remains then at the caller of the lexical analyzer.

This means, that the lexical analyzer *sends* (possibly multiple) tokens
:ref:`sec:usage-sending-tokens` without considering how they are received by
the caller. This section discusses how the tokens are actually handled between
the callee, the generated analyzer, and the caller.

Section :ref:`usage-minimalist-example` already depicted how a caller
of the lexical analyzer initiates a lexical analysis step and how it
receives the token value. In the heart of it is the function call

.. code-block:: cpp

      ...
      token_p = qlex.receive();      
      ...

where a pointer to a token object is passed as a return value.  The lexical
analyzer then fills it with the information for the current analysis step. The
process behind the call to the receive function is called the *token passing
policy*. They are subject of the subsequent sections.

Policy 'Queue'
----------------

Command line option: ``--token-policy  queue``, or no ``--token-policy``. 

This is the default token passing policy, i.e. the policy that is used if no
other one is specified. It is the 'no brainer' since it does require much
consideration. All features of token sending can be used. 

Figure :ref:`Token Passing Policy Queue <fig-token-policy-queue>` shows what
happens behind the scenes. The analyser contains a pointer to the current token
object to be filled.  The ``send()`` functions fill the data into the token
object and increment the pointer. This process continues until, either, the
analyzer reaches a ``return`` statement, or the token queue is filled up.

.. _fig-token-policy-queue:

.. figure:: ../figures/token-policy-queue-2.*

   Token passing policy ``queue`` with function ``receive(Token*)``.
  
The analysis steps are triggered with a call to the member function
``receive(Token*)``.  The user owns a token token object and provides a pointer
to it.  When the function returns, this object is filled with the content 
of the first token in the queue, relying on the ``operator=()``. 
In subsequent calls to ``receive(...)`` it is checked wether the token 
queue contains more tokens and the copying happens directly from the 
qeue. When the token queue is emptied a new analysis step is triggered.

.. _fig-token-policy-queue-pp:

.. figure:: ../figures/token-policy-queue-1.*

   Token passing policy ``queue`` with function ``receive(Token**)``.

The same policy can be applied when the user only wants to own a pointer to a
token (figure :ref:`fig-token-policy-queue-pp`). In this case, a pointer to the
pointer is passed. Where the ``receive(Token*)`` function copied content from
the queue, the function ``receive(Token**)`` only bends the user's pointer.
  
The former call to ``receive()`` passes the ownership of the token to the user,
by copying,  and the analyzer can process the token queue is safely. The
latter call to ``receive()`` is faster, since no copying takes place.
However, it forces to user to process and store the data contained in the
token structure immediately. The next call to ``receive()`` may change
content to what the pointer refers.

Policy 'User's Token'
-----------------------

Command line option ``--token-policy  users_token``.

In this case the user provides a single token. A call to ``receive()``
triggers a single call to the analyzer. The analyzer can only write 
one single token at a time. If it sends twice during a pattern match,
the first content is overwritten.

.. _fig-token-policy-users-token:

.. figure:: ../figures/token-policy-queue-3.*

   Token passing policy ``users_token`` with function ``receive(Token*)``.

This token policy is fast, but requires some care to be taken, as follows:

  #. A pattern action can only send one distinct token!

  #. Event handlers (``on_entry``, ``on_indentation``, etc.) better 
     do not send a token. The 'sending' might interfer with the 
     pattern action that is currently active.

     Any sending of two or more tokens as a reaction to a pattern 
     match may interfere. Avoid those, when using the *user's token*
     policy.

  #. Each time ``receive()`` is called a new token object is to be provided,
     or the content is to be processed immediately.

.. note::

    The policy *user's token* can also be applied with the function call
    ``receive()``, i.e. without any argument. However, then you need to set the
    analyser's member pointer

    .. describe::   .token

       to the token which you provide--either each time when ``receive()`` is
       called, or once at the beginning of the experiment.

