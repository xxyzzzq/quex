The {\quex} program generates a lexical analyser that scans text and
identifies patterns. The result of this lexical analysis is a list of {\it
  tokens}. A token is a piece of atomic information directly relating to a
pattern, or an {\it event}. It consists of a type-identifier, i.e. the {\it
  token type}, and content wich is extracted from the text fragment that
matched the pattern. 

Figure \ref{fig:lexical-analyser} shows the principle of lexical
analysis.  The lexical analyser receives a stream of characters "if( x> 3.1 )
$\{$ ..." and produces a list of tokens that tells what the stream signifies. A first
  token tells that there was an {\tt if} statement, the second token tells
  that there was an opening bracket, the third one tells that there was an
  identifier with the content {\tt x}, and so on. 
  
  In 'real' programming languages the token stream is received by a parser
  that interpretes the given input according to a specific grammar. However,
  for simple scripting languages this token stream might be treated
  immediately. Using a lexical analyser generator for handcrafted ad-hoc
  scripting languages has the advantages that it can be developped faster and
  it is much easier and safer to provide some flexibility and power. This is
  to be demonstrated in the following chapters.

\showpic
{figures/lexical-analysis-process.png}
{The process of lexical analysis}
{fig:lexical-analyser}

The following features distinguish {\quex} from the traditional lexical
analysers such as lex or flex:

\begin{itemize}

\item {\it Ease}. A simple as well as a complicated lexical analyzer can 
      be specified in a very elegant and transparent manner. Do not get confused
      over the set of features and philosophies. If you do not use them, then
      simple skip the concerning regions in the text. Start from the ready-to-rumble
      examples in the ./DEMO subdirectory.

\item A generator for a {\it directly coded lexical analyzer} featuring
  pre- and post-condtions. The generated lexical analyzer is up to 2.5 times
  faster than an analyzer created by flex/lex.
  
\item Sophisticated {\it lexical modes} in which only exclusively specified
  patterns are active. In contrast to normal 'lex' modes they provide
  the following functionality:

  \begin{itemize}
  \item  {\it Inheritance relationships} between lexical analyser modes. This
    allows the systematic inclusion of patterns from other modes, as well as
    convinient transition control.

  \item {\it Mode transition control}, i.e. restriction can be made to what mode
    a certain mode can exit or from which mode it can be entered. This helps 
    to avoid that the lexical analyser drops by accident into an unwanted
    lexical analyses mode.

  \item {\it Mode transition events}, i.e. event handlers can be defined for
    the events of exiting or entering from or to a particular mode.    
    
  \item {\it Indentation events}, i.e it is possible to provide an event handler
    for the event of the first appearing non-whitespace in a line. This event
    handling happens quasi-paralell to the pattern matching.

  \end{itemize}
  
\item A default general purpose {\it token} class. Additionally, {\Quex}
  provides an interface to run the lexical analyser with a user-defined token
  class.
  
\item A {\it token queue} so that tokens can be communicated without returning
  from the analyser function.  The token queue is a key for the production of
  '{\it implicit tokens}', i.e.  tokens that do not relate directly to
  characters in an analysed character stream. Those tokens are derived from
  context. This again, is a key for defining redundancy reduced languages.
  
\item Automatic {\it line} and {\it column numbering}. These features can be
  turned off, in case that one fears performance loss. It can be determined
  excactly from which line and which column a recognized pattern started and
  where it ended.

\end{itemize}

The present text first explains briefly the basic concepts of lexical analysis
in {\quex}. Here, a short review is given on lexical analysis, but then it
concentrates on the introduction of the features mentioned above. The
subsequent chapter dicusses a simple example of a complete application for
lexical analysis. Finally, the last chapter elaborates on the formal usage of
all features of {\quex}. 

%% For those, who only need very quickly a simple lexical analyzer the appendix
%% contains the explanation of a minimalist application.




%% A character stream that is represented as a token list can be understood, i.e.
%% {\it parsed}, relative to a particular {\it grammar}. A grammar defines how
%% the incoming information is going to be arranged. The C-language, for example,
%% says that a sequence $[$"x"$]$,$[$+$]$,$[$"y"$]$ is an addition of two
%% variables that are named 'x' and 'y'. The parsing process directly relates to
%% {\it tree structures} \cite{}. It is tree structures (not table structures)
%% that are used to order non-uniform data. Tree structures can {\it arrange}
%% information from general (at the root) to specific (at the leaves), and tree
%% structures can {\it bracket} information virtually with an arbitrary
%% precision. This makes grammars, and inclusively parsers, the ultimate choice
%% for the interpretation of languages.  Since lexical analysis provides the
%% input for parsing it is an indispensable component in a systematically
%% developped interpreter.

%% The {\quex} program provides functionality to handle those cases. It is
%% designed especially with the ability of {\it implicit tokens} in mind, i.e.
%% tokens that do not relate directly to patterns, but to sequences of tokens or
%% other analysis of the input. The following mechanisms of {\quex} support
%% this concept:


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
