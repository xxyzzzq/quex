.. _sec-token-policies:

Token Passing Policies
======================

The primary result result of a lexical analysis step is a so called 'token
identifier', that tells how the current input can be catagorized, e.g. as a
'KEYWORD', a 'NUMBER', or an 'IP_ADDRESS'. Additionally some information about
the actual stream content might have to be stored. For example when a 'STRING'
is identified the content of the string might be interesting for the caller of
the lexical analyzer. This basic information needs to be passed to the user of
the lexical analyzer. Token passing in quex can be characterized by 

  .. describe:: Memory managements 
  
     Memory management of the token or token queue can be either be
     accomplished by the analyzer engine or by the user.

     By default the analyzer engine takes over memory management for the token
     or token queue. Also, it calls constructors and destructors as required.

  .. describe:: Token passing policy
 
     Possible policies are 'queue' and 'single'.  Tokens can be stored in a
     *queue* to be able to produce more than one token per analyzis step. If
     this is not required a *single* token instance may be used to communicate
     analyzis results.

     The default policy is 'queue' since it is the easiest and safest to handle
     during analyzis and event handlers.

The following two sections discuss automatic and user controlled token memory
management separately. For each memory management type the two token passing
policies 'single' and 'queue' are described.


Automatic Token Memory Management
---------------------------------

Automatic token memory management is active by default. It relieves the user
from anything related to memory management of the internal token or token
queues. It is the seemingless nature which makes it possible to introduce the
concepts of token passing policies without much distractive comments. 


Queue
.....

The default token policy is 'queue'. Explicitly it can be activated
via the command line option ``--token-policy``, i.e.::

   > quex ... --token-policy queue ...

This policy is the safe choice and requires a minimum of programming effort.
It has the following advantages:

   -- Using a token queue enables the sending of multiple
      tokens as response to a single pattern match. 
      
   -- The events ``on_entry``, ``on_exit``, and ``on_indentation`` 
      can be used without much consideration of what happens to the 
      tokens. 
      
   -- Multiple pattern matches can be performed without 
      returning from the lexical analyzer function.

However, there is a significant drawback:

   -- The state of the analyzer is not syncronized with the currently 
      reported token, but with the token currently on top of the queue.

A direct consequence of this is that parameters like line number, stream
position etc.  must be stored inside the token at the time it is created, i.e.
inside the pattern action (see :ref:`sec-token-stamping`). For the above
reason, the line number reported by the analyzer is not necessarily the line
number of the token which is currently taken from the queue.

Figure :ref:`fig-token-queue` shows the mechanism of the token queue policy.
The lexical analyzer fills the tokens in the queue and is only limited by the
queue's size. The user maintains a pointer to the currently considered token
``token_p``. As a result to the call to 'receive()' this pointer bent so that
it points to the next token to be considered. The receive function itself
does the following:

   -- if there are tokens left inside the queue, it returns a pointer
      to the next token.

   -- if not, then a call to the analyzer function is performed. The analyzer
      function fills some tokens in the queue, and the first token from the queue
      is returned to the user. 

An application that applies a 'queue' token policy needs to implement
a code fragment like the following

.. code-block:: cpp

    QUEX_TYPE_TOKEN*  token_p = 0x0; 
    ...
    while( analyzis ) {
        lexer.receive(&token_p);
        ...
        if( content_is_important_f ) {
            safe_token = new QUEX_TYPE_TOKEN(*token_p);
        }
    }

All tokens primarily live inside the analyzer's token queue and the user
only owns a reference to a token object ``token_p``. The next call to
'receive()' may potentially overwrite the content to which ``token_p`` points.
Thus, if the content of the token is of interest for a longer term, then it
must be stored safely away.

.. note:: The usage of ``malloc`` or ``new`` to allocate memory for 
          each token may have a major influence on performance. Consider
          a memory pool instead, which is allocated at once and which
          is able to provide token objects quickly without interaction
          with the operating system.

The size of the token queue is constant during run-time. Quex generates the size
based on the the command line option ``--token-queue-size``. An overflow of the
queue is prevented since the analyser returns as soon as the queue is full. 

Then the ``--token-queue-safety-border`` command line flag allows to define a
safety region. Now, the analyzer returns as soon as the remaining free places
in the queue are less then the specified number. This ensures that for each
analyzis step there is a minimum of places in the queue to which it can write
tokens. In other words, the safety-border corresponds to the maximum number of
tokens send as reaction to a pattern match, including indentation events and
mode transitions.  

For low level interaction with the token queue the following functions are
provided

   .. code-block:: cpp

      bool   token_queue_is_empty();
      void   token_queue_remainder_get(QUEX_TYPE_TOKEN**  begin_p, 
                                       QUEX_TYPE_TOKEN**  end_p);

The first function informs tells whether there are tokens in the queue. The
second function empties the queue. Two pointers as first and second argument
must be provided. At function return, the pointers will point to the begin and
end of the list of remaining tokens. Again, end means the first token after the
last token. Note, that the content from ``begin`` to ``end`` may be overwritten
at the next call to receive(). The following code fragment displays the usage 
of this function:

    QUEX_TYPE_TOKEN*  iterator = 0x0; 
    QUEX_TYPE_TOKEN*  begin_p  = 0x0;
    QUEX_TYPE_TOKEN*  end_p    = 0;
    ...
    while( analyzis ) {
        qlex.receive(&iterator);
        if( content_is_important_f ) {
            store_away(iterator);
        }

        token_queue_take_remainder(&iterator; &end_p);
        while( iterator != end_p ) {
            ...
            if( content_is_important_f ) {
                store_away(iterator);
            }
            ++iterator;
        }
    }

Such a setup may be helpful if the lexical analyzer and the parse run in two
different threads. Then the token tokens that are communicated between the
threads could be copied in greater chunks. 


Single
......

The policy 'single' is an alternative to the 'queue' policy. Explicitly it can
be activated via the command line option ``--token-policy single``, i.e.::

   > quex ... --token-policy single ...

The advantages of a token queue are all lost, i.e. 

   -- No more than one token can be sent per pattern match.
   
   -- Event handlers better not send any token or it must be sure
      that the surrounding patterns do not send token.

   -- Only one pattern match can be performed per call to the analyzer
      function. The usage of ``CONTINUE`` in a pattern action is 
      generally not advisable.

On the other hand, there is a major advantage:

   -- The state of the analyzer is conform with the currently reported
      token.

Thus, line numbers stream positions and even the current lexeme can be
determined from outside the analyzer function. The analyzer function does not
have to send a whole token. Actually, it is only required to send a token id.
This token id is the functions return value and, with most compilers, stored in
a CPU register. Since only one token object is stored inside the analyzer the data
locality is improved and cache misses are less probable.  The token passing
policy 'single' is designed to support a minimal setup, which may 
improve the performance of an analyzer.

.. note:: Even with the token passing policy 'single' the ability to send
          multiple repeated tokens (using ``self_send_n()``) at the same time 
          remains intact. The repetition is implemented via a repetition counter
          not by duplication of tokens.

In contrast to the policy 'queue' a with 'single' the call to ``receive()``
does not bend any token pointer to a current token. Instead, the one 
token inside the analyzer can be referred to once. The receive function
returns only the token id as a basis for later analyzis. If necessary, the
application may rely on the token's content by dereferencing the pointer
at any time. A code fragment for policy 'single' is shown below:

.. code-block:: cpp

    QUEX_TYPE_TOKEN*  token_p = 0x0; 
    ...
    token_p = qlex.token_p();
    ...
    while( analyzis ) {
        token_id = qlex.receive();
        ...
        if( content_is_important_f ) {
            safe_token = new QUEX_TYPE_TOKEN(*token_p);
        }
    }

Again, the next call to ``receive()`` may overwrite the content of the token.
If it is needed on a longer term, then it must be copied to a safe place.

Note, that the function signature for the receive functions in 'queue' and 'single'
are incompatible. The receive function for 'queue' has the following signature

.. code-block:: cpp

    void  receive(QUEX_TYPE_TOKEN**);  // 'queue' bends a pointer

where the signature in case of token passing policy 'single' is

.. code-block:: cpp

    QUEX_TYPE_TOKEN_ID  receive();     // 'single' only reports token id

This choice has been made so that user code breaks if the token policy is switched.
Both policies require a different handling and a compile error forces the user
to rethink his strategy[#f1]_.  It is expected that the compiler reacts to a
mismatching function call by pointing to a location where a matching candidate
can be found. At this location, the user will find a hint that the token policy
is not appropriate. 


User Controlled Token Memory Management
---------------------------------------

The previous discussion has revealed a major drawback in automatic token memory
management: Whenever the content of a token is of interest for a longer term,
it must be copied. This could be spared, if the lexical analyzer is told
were to write the token information. When an analyzis step is done the user
takes the pointer to the finished token, and provides the analyzer with a
pointer to an empty token.  This token-switching  reduces the need for
memory allocation and disposes the need of copying. As a disadvantage, the
user is now responsible for allocating and freeing of memory, as well as
constructing and destructing of the involved tokens. User controlled memory
management is activated via the command line option
``--token-memory-management-by-user``, or ``--tmmbu``, i.e. quex has to 
be called with::

   > quex ... --token-memory-management-by-user ...

The following paragraphs will first discuss the 'single' token passing policy
and then 'queue', because it is straight forward for the former and a little
more involved for the later. Whenever a token is received a new token must be
provided on which the analyzer can write.  Thus, the token pointer switching
and the reception can be combined. The function signature of the receive
function is

.. code-block:: cpp

    QUEX_TYPE_TOKEN* receive(QUEX_TYPE_TOKEN* empty_token);

The receive function takes a pointer to an allocated, ready-to-rumble, but
empty token object. The next analyzis step will write content into this
object. The result of the current analyzis step is available as the return
value. A typical code fragment for this scenerio looks like

.. code-block:: cpp

    QUEX_TYPE_TOKEN  token_bank[2]; 
    QUEX_TYPE_TOKEN* token_p = &token_bank[1];
    ...
    lexer.set_token_p(&token_bank[0]);
    ...
    while( analyzis ) {
        /* call to receive(...) switches token pointer */
        token_p = lexer.receive(token_p);
        ...
        if( content_is_important_f ) {
            store_away(token_p);
            token_p = get_new_token();
        }
    }

The idea of 'switching the thing on which the analyzer writes' can also be
applied to the 'queue' token passing policy. The user provides a chunk of
allocated and constructed tokens. The receive function fills the token queue
during the call to receive and prepares a new token queue to carry analyzis
results. The switched entity is the token queue and no longer a single token.
Note, that with this approach, the receive function can longer longer
implicitly iterate over a token queue in the background. Instead, the user is
responsible to iterate over the received queue of tokens. The function
signature of the receive function is the following:

.. code-block:: cpp

    void   receive(QUEX_TYPE_ANALYZER*  me,
                   QUEX_TYPE_TOKEN**    finished_token_queue, 
                   QUEX_TYPE_TOKEN**    finished_token_queue_watermark, 
                   QUEX_TYPE_TOKEN*     empty_token_queue,
                   size_t               empty_token_queue_size);

The resulting token queue is communicated via the return value and the 
first argument. The first argument's pointer is bent to the begin of the
currently finished token queue and the return value marks the end of the
queue. Further, the receive function accepts a new token queue for the next
analyzis step by means of the second and third argument, carrying a pointer
to the begin of the new empty token queue and its size.

This receive function always performs a call to the underlying analyzer
function. At each step an empty queue is filled from ground until the analyzer
function returns. A typical code fragment applying this approach could be
similar to 

.. code-block:: cpp

    QUEX_TYPE_TOKEN*  iterator        = 0x0; 
    QUEX_TYPE_TOKEN*  queue           = 0x0;
    QUEX_TYPE_TOKEN*  queue_watermark = 0;
    ...
    empty_queue = get_new_token_queue(empty_queue_size);
    qlex.set_token_queue(empty_queue, empty_queue_size);
    ...
    while( analyzis ) {
        empty_queue      = get_new_token_queue(empty_queue_size);
        qlex.receive(&queue, &queue_watermark, empty_queue, empty_queue_size);

        /* Manual iteration over received token queue */
        for(iterator = queue; iterator != queue_watermark ; ++iterator) {
            ...
            if( content_is_important_f ) {
                store_away(iterator);
            }
        }
    }

Additionally, there are three functions for low-level interaction with the
token queue:

   .. code-block:: cpp

      void   token_queue_memory_switch(QUEX_TYPE_TOKEN** memory, size_t* n);

Analogously in C:

      void   QUEX_NAME(token_queue_memory_switch)(QUEX_TYPE_ANALYZER* me,
                                                  QUEX_TYPE_TOKEN**   memory, 
                                                  size_t*             n);

The first function returns ``true`` if the queue is empty and ``false`` if
not. The second function returns the remaining tokens. After a call to 
this function ``*Begin`` will point to the begin of the remaining tokens
and ``*End`` will point right behind the last token in the remainder. Thus,
it is possible to iterate over the remaining tokens with

   .. code-block:: cpp

      qlex.token_queue_remainder_get(&begin, &end);
      for(QUEX_TYPE_TOKEN* p = *begin; p != *end; ++p) {
          ...
      }

Note, that a call to this function empties the token queue. Thus, if one does
not consider the reported remaining tokens, then tokens are definitely lost.
The function ``token_queue_memory_switch(...)`` sets a memory chunk that is
to be used as a token queue. At the same time it returns the pointer to the
previous memory. Its arguments are as follows:

  .. describe:: ``QUEX_TYPE_TOKEN** memory``

     At function entry ``*memory`` must point to the memory chunk to be used
     for the token queue. At function exit ``*memory`` will point to the 
     chunk that was used before. The ownership of the memory chunk is
     now transferred to the caller, i.e. the user. He is now responsible
     of deleting it. If ``*memory`` is zero at function exit then there
     was not yet any memory associated.

  .. describe:: ``size_t* n``

     At function entry ``*n`` must contain the numbers of tokens that fit into
     the provided chunk. At function exit ``*n`` will contain the number of
     tokens carried by the previous memory chunk.
     
The design choice of implementing a 'switch' function instead of a 'set' and a
'get' function was made by purpose. It imposes on the user to have an idea
about what to do with the memory chunk that is currently used. If only the new
memory chunk was put in place and no one takes responsibility of the old one,
then this is a source of memory leaks. By default the ability to set a
customized memory chunk is turned of. It is only enabled via if the
macro::

        QUEX_OPTION_USER_MANAGED_TOKEN_MEMORY

e.g. witha a command line ``-D...`` definition, or the
command line flag ``--token-memory-management-by-user`` or ``--tmmbu`` being
passed to quex. If user managed token memory is defined, no initial token queue
is setup. The user is responsible for allocating the memory chunk and for
calling the constructors for each token element. Vice versa, the analyzer's
destructor will neither take care of the destruction of the token objects, nor it
will deallocate the memory chunk where they reside. It is all the users task.

It is advisable to call ``token_queue_memory_get(...)`` where ``*memory`` will
point to the begin of the token queue's memory and ``*n`` will contain the
number of tokens that can be contained by it.

It is generally a good idea to check whether the token queue is empty before
imposing a new queue. That is

   .. code-block:: cpp

      if( my_lexer.token_queue_is_empty() ) {
          n      = 32;
          memory = new QUEX_TYPE_TOKEN[n];

          my_lexer.token_queue_memory_switch(&memory, &n);

          if( memory != 0x0 ) delete [] memory;
      }

otherwise, one might risk to drop tokens that are lost inside the old queue
when the new queue came in. As shown in the example it shall also be checked
wether the memory was already set. If so the user is responsible for deleting
it.  The memory passed as token queue memory must be allocated and initialized,
i.e. the constructor must have been called for each token object in the
memory chunk. In C++ this happens automatically when the ``new`` operator
is used for arrays, e.g.

   .. code-block:: cpp

      QUEX_TYPE_TOKEN*  my_memory = new QUEX_TYPE_TOKEN[512];

does all that is required to setup a token queue memory. In C, though, 
the constructor calls for the tokens must be triggered explicitly, e.g.

   .. code-block:: cpp

      size_t             n             = 512;
      QUEX_TYPE_TOKEN*   my_memory     = 0x0;
      QUEX_TYPE_TOKEN*   my_memory_end = 0x0;

      my_memory     = (QUEX_TYPE_TOKEN*)malloc(512 * sizeof(QUEX_TYPE_TOKEN));
      my_memory_end = my_memory + n;

      for(p = my_memory; p != my_memory_end; ++p) {
          QUEX_NAME_TOKEN(construct)(p);
      }

      QUEX_NAME(token_queue_memory_switch)(&my_memory, &n);
      ...

is a C code fragment that corresponds to the C++ fragment above. Analogously,
the destruction of a token queue array happens automatically C++. In C the destructors
for the token objects must be called explicitly before the memory can be freed, e.g.

   .. code-block:: cpp

      if( my_memory != 0x0 ) {
          for(p = my_memory; p != my_memory_end; ++p) {
              QUEX_NAME_TOKEN(destruct)(p);
          }
          free(my_memory);
      }

.. warning::

   The state of the lexical analyzer corresponds always to the last token in
   the token queue! The ``.receive()`` functions only take one token from the
   queue which is not necessarily the last one. If state elements, such as line
   numbers or indentation, need to be associated with tokens, token stamping
   needs to be used :ref:`sec-token-stamping`.

Single
------

Command line option: ``--token-policy  single``

When a single token is used many advantages are lost queue are lost. The
capability to send repeated tokens via a single token using ``self_send_n``
remains intact. However, only one distinct token can be sent as reaction to one
pattern match. Also, it must be taken care that the pattern actions that appear
around the events ``on_entry``, ``on_exit``, and ``on_indentation`` do not sent
tokens if the event handler's do. Otherwise, token content is simply overwritten
and lost. Also, if the content of a token is set by a pattern action, it is
generally a bad idea to use ``CONTINUE`` after sending a token. The subsequent
pattern action may corrupt the token content. 

   .. code-block:: cpp

      token_id = my_lexer.receive();

where the member function ``.receive()`` returns the identifier of the current
token. The current token can be accessed via

   .. code-block:: cpp
   
       my_lexer.token_p(&token_p);

where the returned pointer always points to the same token object as long
as one does not call 

   .. code-block:: cpp
   
       prev_token_p = my_lexer.token_p_switch(&my_token);

where the passed argument must point to a prepared token object. Again, the
choice of implementing a 'switch' function was made to remind the user of the
previous token inside the analyzer of which he needs to take care. If the
returned pointer is zero, then no token was previously set. Otherwise, the user
takes over the ownership over the token and is responsible for destructing and
de-allocating it. That is, for example

   .. code-block:: cpp

       token_p = (QUEX_TYPE_TOKEN*)malloc(sizeof(QUEX_TYPE_TOKEN));
       QUEX_NAME_TOKEN(construct)(token_p);

       prev_token_p = QUEX_NAME(token_p_switch)(token_p);
       if( prev_token_p != 0x0 ) {
           QUEX_NAME_TOKEN(destruct)(prev_token_p);
           free(prev_token_p);
       }

is an appropriate way to handle multiple tokens in plain C. In C++ things 
are a lot easier. The above segment can be written in C++ as

   .. code-block:: cpp

       prev_token_p = QUEX_NAME(token_p_switch)(new QUEX_TYPE_TOKEN());
       if( prev_token_p != 0x0 ) delete prev_token_p;

This is so, since in C++ constructor and destructor calls happen implicitly
when the operators ``new`` and ``delete`` are applied.  As with the token queue,
this set funtion is only available if the macro::

        QUEX_OPTION_USER_MANAGED_TOKEN_MEMORY

is defined, or the command line option ``--token-memory-management-by-user``
or ``--tmmbu`` is passed to quex. As with the policy 'queue' the token memory
needs to be constructed. In C++ this happens implicitly by a call to the 
``new``-operator or by plain variable definition. In C the token object
needs to be constructed explicitly, i.e. a fragment such as the following 
is required

   .. code-block:: cpp

       QUEX_TYPE_TOKEN   my_token;
       ...
       QUEX_NAME_TOKEN(construct)(&my_token);
       ...
       my_lexer.token_p_set(&my_token);

Again, the destruction of a token object happens implicitly in C++, but 
in C the destructor must be called explicitly, i.e.

   .. code-block:: cpp

      QUEX_NAME_TOKEN(destruct)(&my_token);

must be called befor the token object is deallocated or runs out of scope, i.e.
before function return if it is defined as a local variable. 

An advantage of the single token policy is that there is no increment 
operation for the iteration over the token array. Also, no token queue
reset needs to happen at the entry of the analyzis step. A decision whether
to take the risk and effort that comes with the single token policy
needs to be well considered. It is always a good idea to run some benchmarks
on a basic lexical analyzer implementation in order to see which policy
is preferrable. 

Remarks
-------

The passing of tokens from the analyzer is a straight-forward one-way
process. The life line of an object generated by the analyzer shall
look like

   #. Caller calls the lexical analyzer via ``.receive()``.
   #. Analyzer generates an object based on the lexical 'event'. It does
      *not* create the token that carries it.
   #. Object is stored inside the token.
   #. Caller takes the object.
   #. Caller deletes the object, but *not* the token.

If this policy is followed consequently, then the token never needs to have
ownership over the carried object. That means, it shall not be responsible for
allocation or deallocation of the carried object.  As a consequence the token
passing becomes both faster and less memory consuming. The default token class
used in quex, however, implements a safe strategy. For example the ``text``
member relies on C++ Standard Library's ``basic_string`` for strings. This
ensures that the carried string object is deleted as soon as it is re-assigned
or if the token is destructed at destruction time of the analyzer. A well
designed token passing might cope with simple pointers to memory that is
allocated by the analyzer and de-allocated by the caller of the analyzer.

.. note:: 

   The allocation, constructor calls, destructor calls  or de-allocation of
   token memory and tokens *is not* element of the analyzis process! The tokens
   are created once--at best before the analyzis starts. The are equally
   deleted once when the analyzis is terminated. Tokens behave like ships
   that are kept ready to carry information from the analyzer to the caller
   of the analyzer.

Summary
-------

Token policies enable a fine grain adaption of the lexical analyzer. They
differ in their efficiency in terms of computation speed and memory
consumption. Which particular policy is preferable depends on the particular
application. Token queues are easier to handle since no care has to be 
taken about tokens being sent from inside event handlers and multiple tokens
can be sent from inside a single action without much worries. Token queues
require a little more memory and a little more computation time than 
single tokens. Token queue can reduce the number of function calls since
the analyzis can continue without returning until the queue is filled.
Nevertheless, only benchmark tests can produce quantitative statements
about which policy is preferable for a particular application.

.. note:: 

   The implicit repetition of tokens is available for both policies.  That is,
   ``self_send_n()`` is always available. The requirement that a multiple same
   tokens may be sent repeatedly does *not* imply that a token queue policy must be
   used.

.. rubric:: Footnotes

   [#f1] Previous designs where both setups compiled for both policies had
   let to wrong and confusing results; and only in depth analyzis revealed the
   inappropriate token policy being applied. 

