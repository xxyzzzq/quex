.. _sec-token-policies:

Token Passing Policies
======================

The primary result result of a lexical analysis step is a so called token
identifier, that tells how the current input can be catagorized, e.g. as
a 'KEYWORD', a 'NUMBER', or an 'IP_ADDRESS'. Additionally some information
about the actual stream content might have to be stored. For example 
when a 'STRING' is identified the content of the string might be interesting
for the caller of the lexical analyzer. Thus, for the general case,
there must be some type of container that allows to store information
about the current token beyond his token identifier. Here comes a very
important point about the passing of this information.

.. quote::

   The information contained in a token *is not* owned by the
   token itself, in a sense that it is *not* responsible for 
   de-allocating its content. A token only carries values and
   pointers from the lexical analyzer to its caller and the
   ownership remains then at the caller of the lexical analyzer.

This means, that the lexical analyzer *sends* (possibly multiple) tokens
:ref:`sec:usage-sending-tokens` without considering how they are received by
the caller. This section discusses how the tokens are actually handled between
the callee, the generated analyzer, and the caller.

Section :ref:`usage-minimalist-example` already depicted how a caller
of the lexical analyzer initiates a lexical analysis step and how it
receives the token value. In the heart of it is the function call

.. code-block:: cpp

      ...
      token_p = qlex.receive();      
      ...

where a pointer to a token object is passed as a return value. 
The lexical analyzer then fills
it with the information for the current analysis step. The process behind the
call to the receive function is called the *token passing policy*. They
are subject of the subsequent sections.

Let ``Token`` be a shorthand for ``QUEX_TYPE_TOKEN`` in the following 
subsections.

.. warning::

   When using token queue: The state of the lexical analyzer
   corresponds to the last token in the token queue! The ``.receive()``
   functions only take one token from the queue which is not necessarily the
   last one. If state elements, such as line numbers or indentation, need to be
   associated with tokens, token stamping needs to be used
   :ref:`sec-token-stamping`.

.. _advanced-safety-border:

Policy 'Queue'
----------------

Command line option: ``--token-policy  queue``, or no ``--token-policy``. 

This is the default token passing policy, i.e. the policy that is used if no
other one is specified. It is the 'no brainer' since it does require much
consideration. All features of token sending can be used. 

Figure :ref:`Token Passing Policy Queue <fig-token-policy-queue>` shows what
happens behind the scenes. The analyser contains a pointer to the current token
object to be filled.  The ``send()`` functions fill the data into the token
object and increment the pointer. This process continues until, either, the
analyzer reaches a ``return`` statement, or the token queue is filled up.

.. _fig-token-policy-queue:

.. figure:: ../figures/token-policy-queue-2.*

   Token passing policy ``queue`` with function ``receive(Token*)``.
  
The analysis steps are triggered with a call to the member function
``receive(Token*)``.  The user owns a token token object and provides a pointer
to it.  When the function returns, this object is filled with the content 
of the first token in the queue, relying on the ``operator=()``. 
In subsequent calls to ``receive(...)`` it is checked wether the token 
queue contains more tokens and the copying happens directly from the 
qeue. When the token queue is emptied a new analysis step is triggered.

.. _fig-token-policy-queue-pp:

.. figure:: ../figures/token-policy-queue-1.*

   Token passing policy ``queue`` with function ``receive(Token**)``.

The same policy can be applied when the user only wants to own a pointer to a
token (figure :ref:`fig-token-policy-queue-pp`). In this case, a pointer to the
pointer is passed. Where the ``receive(Token*)`` function copied content from
the queue, the function ``receive(Token**)`` only bends the user's pointer.
  
The former call to ``receive()`` passes the ownership of the token to the user,
by copying,  and the analyzer can process the token queue is safely. The
latter call to ``receive()`` is faster, since no copying takes place.
However, it forces to user to process and store the data contained in the
token structure immediately. The next call to ``receive()`` may change
content to what the pointer refers.

The size of the token queue is constant during run-time. Quex generates the size
based on the the command line option ``--token-queue-size``. An overflow of the
queue is prevented since the analyser returns as soon as the queue is full. The
default size of the token queue is 64 tokens.

In case that multiple tokens are sent in an action, then the
``--token-queue-safety-border`` command line flag allows to define a safety
region. Now, the analyzer returns as soon as the remaining free places in the
queue are less then the specified number. This ensures that any action find a
minimum of places in the queue to which it can write tokens. In other words,
the safety-border corresponds to the maximum number of tokens send as
reaction to a pattern match, including indentation events and mode
transitions.  The default safety border is 16 tokens.

Note, the last token may be repeated ``N`` times, when the maximum number
``N`` is limited by the definition of ``size_t`` of the operating system
environment.

Policy 'User's Token'
-----------------------

Command line option ``--token-policy  users_token``.

In this case the user provides a single token. A call to ``receive()``
triggers a single call to the analyzer. The analyzer can only write 
one single token at a time. If it sends twice during a pattern match,
the first content is overwritten.

.. _fig-token-policy-users-token:

.. figure:: ../figures/token-policy-queue-3.*

   Token passing policy ``users_token`` with function ``receive(Token*)``.

This token policy is fast, but requires some care to be taken, as follows:

  #. A pattern action can only send one distinct token!

  #. Event handlers (``on_entry``, ``on_indentation``, etc.) better 
     do not send a token. The 'sending' might interfer with the 
     pattern action that is currently active.

     Any sending of two or more tokens as a reaction to a pattern 
     match may interfere. Avoid those, when using the *user's token*
     policy.

  #. Each time ``receive()`` is called a new token object is to be provided,
     or the content is to be processed immediately.

.. note::

    The policy *user's token* can also be applied with the function call
    ``receive()``, i.e. without any argument. However, then you need to set the
    analyser's member pointer

    .. describe::   .token

       to the token which you provide--either each time when ``receive()`` is
       called, or once at the beginning of the experiment.

