\newcommand\clopt[4]{
    \item[\tt #1 {\it #2\/}:] \hfill default={\tt #3}

    #4
}

Main options to control the behavior of the generated lexical analyzer are:

\begin{description}

    \clopt{"-i", "--mode-files"} {file-list} {""}
        {
	    {\it file-list} = list of files of the file containing mode definitions
	    (see sections \ref{sec:practical/modes},
	     \ref{sec:practical/pattern-action-pairs}, and
	     \ref{sec:formal/generated-class/mode-handling})
	}

    \clopt{"--token-prefix"} {name} {TKN\_}
	{
	    {\it name} = Name prefix that is to be sticked in front of any name
	    given in the token-id files. For example, if a token-id file contains
	    the name '{\tt COMPLEX}' and the token-prefix is '{\tt TOKEN\_}'
	    then the token-id inside the code will be '{\tt TOKEN\_COMPLEX}'.
	}

    \clopt{"--token-offset"} {number} {10000}
        {
	    {\it number} = Number where the numeric values for the token-ids start
	    to count.
	}

    \clopt{"--version-id"} {name} {0.0.0-pre-release}
        { 
	    {\tt name} = arbitrary name of the version that was generated. This string
	    is reported by the {\tt version()} member function of the lexical analyser.
	}

    \clopt{"--foreign-token-id-file"} {filename} {""}
        {
	    {\it filename} = Name of the file that contains an alternative definition
	    of the numerical values for the token-ids (see also section
	    \ref{sec:formal/macro}).
	}

    \clopt{"-o", "--engine"}{name}{lexer}    
        {
	    {\it name} = Name of the lexical analyser class that is to be created
	    inside the namespace {\tt quex}. This name also determines the
		filestem of the output files: {\it name}, {\it name}-{\tt internal.h}, 
	    {\it name}-{\tt token\_ids}, and {\it name}{\.cpp}.
        }

    \item{"--debug"} {} {False}
        { 
	    If provided, then code fragments are created to
	    activate the output of every pattern match. Then defining the macro {\tt
	    DEBUG\_QUEX\_PATTERN\_MATCHES} activates those printouts in the
	    standard error output. Note, that this options produces considerable
	    code overhead. If '{\tt no}' is specified, then no such code is created.
        }       
   \item{"--no-mode-transition-check} {} {False}
        {
	    Turns off the mode transition check and makes the engine a little faster.
	    During development this option should not be used. But the final lexical
	    analyzer should be created with this option set.
	}
\end{description}

For the support of derivation from the generated lexical analyzer class the
following command line options can be used.

\begin{description}
    \clopt{"--derived-class"} {name} {$<$none$>$} 
        {
	    {\tt name} = If specified, name of the derived class that the user intends to provide
	    (see section \ref{sec:formal/derivation}). Note, specifying this option
	     signalizes that the user wants to derive from the generated class. If this
	     is not desired, this option, and the following, have to be left out.
        }        
        
    \clopt{"--derived-class-file"} {filename} {""} 
        {
	    {\it filename} = If specified, name of the file where the derived class is
	    defined.  This option makes only sense in the context of option {\tt
		--derived-class}.
        }

    \clopt{"--friend-class"} {name-list} {} 
        {
	    {\it name-list} = Names of classes that shall be friends of the
	    generated lexical analyser. This is to be used, if other classes 
		need to have access to protected or private members of the      
		analyser. It is only to be used by specialists.
	}
\end{description}


Additionally, there are options for specialists who want to provide their own
token-class:

\begin{description}

    \clopt{"--token-class"} {name} {token}
        {
	    {\it name} = Name of the token class that the user defined. Note that the 
	    token class needs to be specified in namespace {\quex}.
        }

    \clopt{"--token-class-file"} {filename} {\$(QUEX\_PATH)/code\_base/token}
        {
	    {\it filename} = Name of file that contains the definition of the
	    token class.
        }
\end{description}

Even if a non-{\quex} token class is provided, still the token-id generator may
be useful. By default, it remains in place. The user, however, can specify the
following option to disable it:

\begin{description}
    \clopt{"--user-token-id-file"} {filename} {$<$none$>$}
        {
	    {\it filename} = Name of file that contains the definition of the
	    token-ids and the mapping function from numerical token-ids to {\tt
	    std::string} objects, i.e. human readable names.
	}
\end{description}

There may be cases where the characters used to indicate end-of-stream and
begin-of-stream need to be redefined, because the default code points appear in
a pattern\footnote{As for 'normal' ASCII or Unicode based lexical analyzers,
    this would most probably not be a good design decision. But, when other,
    alien, non-unicode codings are to be used, this case is conceivable.}.
    Also, note that the '.' regular expression (meaning 'nothing but newline or
	    end of file') needs check for begin-of-buffer and end-of-buffer in
    the general case. Giving both the same value may come with some speedup,
    and does not hurt. None of these values should be zero, because this is the buffer
    limit delimiter. Then the following options allow to modify those values:

\begin{description}
    \clopt{"--begin-of-stream"} {number} {0x19}
          {}
    \clopt{"--end-of-stream"} {number} {0x1A}
          {}
\end{description}

The numbers following these options can be either decimal, without any prefix,
    hexadecimal with a '0x' prefix or octal with a '0o' prefix.  If the trivial
    end-of-line pre-condition (i.e. the '\$' at the end of a regular
	    expression) is sused and the lexical analyzer has to run on a
    system that codes newline as hexadecimal '0D.0A' then you need to specify
    the following option:

\begin{description}
    \clopt{"--DOS"} {} {False}
          {}
\end{description}

For unicode support it is essential to allow iconv support \cite{}. For this 
the iconv library must be installed on your system. On Unix systems this library
is usually present. If a coding other than ASCII is required, specify the following
options:

\begin{description}
    \clopt{--iconv} {} {False}
        {
	    Enable the use of the iconv library for character stream decoding. This option
            is a {\it must} for unicode support.
	}
    \clopt{--bytes-per-ucs-code-point", "-b"} {$[$1, 2, 4$]$, or {\tt wchar\_t}} {2}
        {
	    With this option the internal representation of character is specified. It 
            determines the byte number per character which compose the lexeme strings
	    and on which the lexical analyzer engine internally operates. The byte number
	    should at least suffice to carry the desired input coding space. You can
	    only specify 1 byte, 2 byte or 4 byte per character. If {\tt wchar\_t} is specified
	    {\quex} automatically adapts to the correspondent type of the  operating
	    system environment where the target code is compiled. 
	    Use this, if option {\tt --iconv} is used and you are in doubt.
	}
    \clopt{--endian}{"little", "big"}{"<system>"}
        {
	    There are two types of byte ordering for integer number for different CPUs.
	    For creating a lexical analyzer engine on the same CPU type as {\quex} runs
	    then this option is not required, since {\quex} finds this out by its own.
	    If you create an engine for a different plattform, you must know its byte ordering
	    scheme, i.e. little endian or big endian, and specify it after '{\tt --endian}'.
	}
\end{description}


Note, that for well tempered patterns this should be safe even on Unix
machines. The generated lexical analyzer, though, might be a litter slower.
For version information pass option '{\tt --version}' or '{\tt -v}'. The
options '{\tt --help} and '{\tt -h}' are reserved for requesting a help text.

