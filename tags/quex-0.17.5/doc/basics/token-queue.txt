Quex promotes the idea of implicit tokens, i.e. tokens that are produced
without any pattern match. This concept raised from the fact that some tokens
are, in fact, redundant and can be derived from context. From the perspective
of the source code appearance of a programming language, a redundancy free
language is much clearer than one that forces the programmer to specify
information over and over again. Consider the example of the
\verb|\|begin-\verb|\|end-regions in {\LaTeX} as shown in the subsequent listing. 

[latex]
source~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{itemize}
\item this is the first item.
\item this is the second one.
...
\item this is the last item.
\end{itemize}
source~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Logically, the begin and end tokens could have been derived from the fact that
an \verb|\|item token pops up in normal text mode.  Implicit tokens means
   that the lexical analyser can read between the lines.  It sees things that
   are meant to be there from the context.  Implementing the concept of
   implicit token generation inside lexical analysis rather than in a parsers
   allows the parser to constitute a unit that reasons logically, i.e. it
   considers distinct signals as input, it has a clear set of grammatical
   rules, and produces a clear understanding of the information (e.g. a syntax
	   tree of a program). In _Quex_'s philosophy  words that are
   implicitly thought to be _inserted_ into the context of a sentence, are
   treated by lexical analysis.  _Multiple-Meanings_ (TODO: check this
	   wording), i.e. the _interpretation_ of a signal/token due to its
   context is expressed by grammar. 

Traditionally, the parser calls a function of the form 'get\_token()';
the lexical analyser does one single pattern match, and then returns
the token that represents the matched pattern---or an error, if no pattern matched. 
This mechanism is not sufficient when implicit tokens are involved, because implicit tokens can 
pop up out of nowhere in the character stream. When a pattern matches
a whole set of such implicit tokens may be ready to be sent.

.Sequence diagram displaying the token-passing from the pattern-match action to the caller of `get\_token()`.
image::figures/token-queue.png[fig:token-queue]

Figure <<fig:token-queue, style=ref>> shows the information flow when a lexical
analyser generated by quex parses a character stream.  The user begins and
requests a new token using member function `get\_token()`.  The engine's
object start then the lexical analysis. During the analysis, the pattern
matching actions may send a whole set of tokens into the _token-queue_.
When the pattern matching actions decide to come back, the `get\_token()`
function returns the first token that was entered into the queue.

The subsequent call to `get\_token()`, though, does not trigger a lexical
analysis since there are still tokens in the queue. Until the queue is empty, 
the function only returns tokens from the queue. When the queue is empty,
the lexical analysis starts again in the same manner as described above. 
From the caller's side the process appears to be a reading from a continuous
token stream. From inside the pattern match action the process appears
to be a sending process, where tokens are sent to some recipient.


