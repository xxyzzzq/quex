Quex promotes the idea of implicit tokens, i.e. tokens that are produced
without any pattern match. This concept raised from the fact that some tokens
are, in fact, redundant and can be derived from context. From the perspective
of the source code appearance of a programming language, a redundancy free
language is much clearer than one that forces the programmer to specify
information over and over again. Consider the example of the
\verb|\|begin-\verb|\|end-regions in {\LaTeX} as shown in the subsequent listing. 

[latex]
source~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
\begin{itemize}
\item this is the first item.
\item this is the second one.
...
\item this is the last item.
\end{itemize}
source~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

Logically, the begin and end tokens could have been derived from the fact that
an \verb|\|item token pops up in normal text mode.  Implicit tokens means
   that the lexical analyser can read between the lines.  It sees things that
   are meant to be there from the context.  Implementing the concept of
   implicit token generation inside lexical analysis rather than in a parsers
   allows the parser to constitute a unit that reasons logically, i.e. it
   considers distinct signals as input, it has a clear set of grammatical
   rules, and produces a clear understanding of the information (e.g. a syntax
	   tree of a program). In _Quex_'s philosophy  words that are
   implicitly thought to be _inserted_ into the context of a sentence, are
   treated by lexical analysis.  _Multiple-Meanings_ (TODO: check this
	   wording), i.e. the _interpretation_ of a signal/token due to its
   context is expressed by grammar. 

Traditionally, the parser calls a function of the form 'get\_token()';
the lexical analyser does one single pattern match, and then returns
the token that represents the matched pattern---or an error, if no pattern matched. 
This mechanism is not sufficient when implicit tokens are involved, because implicit tokens can 
pop up out of nowhere in the character stream. When a pattern matches
a whole set of such implicit tokens may be ready to be sent.

.Sequence diagram displaying the token-passing from the pattern-match action to the caller of `get\_token()`.
image::figures/token-queue.png[fig:token-queue]

Figure <<fig:token-queue, style=ref>> shows the information flow when a lexical
analyser generated by quex parses a character stream.  The user begins and
requests a new token using member function `get\_token()`.  The engine's
object start then the lexical analysis. During the analysis, the pattern
matching actions may send a whole set of tokens into the _token-queue_.
When the pattern matching actions decide to come back, the `get\_token()`
function returns the first token that was entered into the queue.

The subsequent call to `get\_token()`, though, does not trigger a lexical
analysis since there are still tokens in the queue. Until the queue is empty, 
the function only returns tokens from the queue. When the queue is empty,
the lexical analysis starts again in the same manner as described above. 
From the caller's side the process appears to be a reading from a continuous
token stream. From inside the pattern match action the process appears
to be a sending process, where tokens are sent to some recipient.

An important discussion evolves around what type  of variables should be
supported by the token that is sent. The author of quex, prefers the idea
of separating the interpretation of the lexeme from lexical analysis. This
is for the following reasons:

Clarity of Concept::

    - The operation, such as 'convert to double' needs to be _imported_ somehow
      into the lexical analysis framework. The lexical analyser by itself
      has no knowledge how to do that.

    - The token type becomes polymorph. That means that it has to support multiple
      data types. This increases complexity of handling this simple data structure.

    - A separate layer of 'interpretation' increases clarity.

Computational Issues::

    - An interpretation creates an object, i.e. the result of the interpretation of 
      the lexeme. This object is possibly not used by the parser. Thus the 
      creation, initialization and destruction of this object is wasted effort.
      If the parser controls the interpretation these redundant operations 
      can be avoided.

    - It is particularly hard to define a data structure that can carry multiple
      types of objects, possibly of different sizes, that is memory and computationally
      efficient.

    - If the interpreted objects posses a some more complex structure, e.g. pointers
      to related objects, then the data locality increases. That means that the 
      probability of a cache-miss increases. This can have a dramatic impact on 
      the computational speed.

On the other hand, there might be 'hints' that the lexer gives about a token. The 
author did not come up with _the perfect token class_, but instead leaves it
open to be redefined by the user for his particular needs.
